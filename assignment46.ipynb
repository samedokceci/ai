{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Data Loading, Preprocessing, and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% Setup: Imports and Global Parameters\n",
    "import os\n",
    "import cv2 # OpenCV for image manipulation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# tensorflow.keras.utils.to_categorical for one-hot encoding, can be added later if needed for NNs\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE # For t-SNE visualization\n",
    "from sklearn.preprocessing import StandardScaler # For t-SNE and feature scaling\n",
    "import time # For retrying image reads\n",
    "from PIL import Image # For robust image opening and format checks\n",
    "\n",
    "# --- USER CONFIGURABLE PARAMETERS ---\n",
    "# !!! IMPORTANT: SET THIS TO YOUR DATASET PATH !!!\n",
    "DATASET_BASE_DIR = 'Birds_25'  # Path to your 'Birds_25' directory\n",
    "\n",
    "# Image dimensions for resizing and storing in memory (BGR format)\n",
    "IMG_WIDTH = 128 #\n",
    "IMG_HEIGHT = 128 #\n",
    "IMG_CHANNELS = 3 # Images will be stored as BGR\n",
    "\n",
    "NUM_CLASSES = 25 # As per the assignment\n",
    "# --- END USER CONFIGURABLE PARAMETERS ---\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATASET_BASE_DIR, 'train') #\n",
    "VALID_DIR = os.path.join(DATASET_BASE_DIR, 'valid') #\n",
    "\n",
    "print(f\"Image dimensions for in-memory storage: {IMG_WIDTH}x{IMG_HEIGHT} (BGR)\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Training directory: {TRAIN_DIR}\")\n",
    "print(f\"Validation directory (original): {VALID_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = []\n",
    "if os.path.exists(TRAIN_DIR):\n",
    "    species_list = sorted([d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))]) #\n",
    "else:\n",
    "    print(f\"ERROR: Training directory not found at '{TRAIN_DIR}'. Please check DATASET_BASE_DIR.\")\n",
    "    raise FileNotFoundError(f\"Training directory not found: {TRAIN_DIR}\")\n",
    "\n",
    "if not species_list:\n",
    "    print(\"ERROR: Species list is empty. Ensure dataset is structured correctly.\")\n",
    "else:\n",
    "    print(f\"Found {len(species_list)} species. First 5: {species_list[:5]}...\") #\n",
    "    if len(species_list) != NUM_CLASSES:\n",
    "        print(f\"Warning: Discovered {len(species_list)} species, but NUM_CLASSES is set to {NUM_CLASSES}. Will use discovered count: {len(species_list)}\")\n",
    "        NUM_CLASSES = len(species_list)\n",
    "\n",
    "all_original_train_paths = [] #\n",
    "all_original_train_labels_str = [] #\n",
    "all_original_valid_paths = [] #\n",
    "all_original_valid_labels_str = [] #\n",
    "\n",
    "for species_name in species_list:\n",
    "    species_train_dir = os.path.join(TRAIN_DIR, species_name) #\n",
    "    if os.path.isdir(species_train_dir):\n",
    "        for img_file in os.listdir(species_train_dir): #\n",
    "            if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')): # Added more common extensions\n",
    "                all_original_train_paths.append(os.path.join(species_train_dir, img_file)) #\n",
    "                all_original_train_labels_str.append(species_name) #\n",
    "\n",
    "    species_valid_dir = os.path.join(VALID_DIR, species_name) #\n",
    "    if os.path.isdir(species_valid_dir):\n",
    "        for img_file in os.listdir(species_valid_dir): #\n",
    "            if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')): #\n",
    "                all_original_valid_paths.append(os.path.join(species_valid_dir, img_file)) #\n",
    "                all_original_valid_labels_str.append(species_name) #\n",
    "\n",
    "print(f\"\\nTotal original training image paths collected: {len(all_original_train_paths)}\") #\n",
    "print(f\"Total original validation image paths collected: {len(all_original_valid_paths)}\") #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# This step prepares the lists of paths and numerically encoded labels for each dataset split.\n",
    "# %% Step 2 Code\n",
    "if not all_original_train_labels_str:\n",
    "    print(\"ERROR: No training labels found from Step 1. Cannot proceed with label encoding.\")\n",
    "    # Handle error or ensure Step 1 ran correctly and found images.\n",
    "    label_encoder = LabelEncoder() # Initialize to prevent later errors, but it won't be fit.\n",
    "    label_mapping = {}\n",
    "    all_original_train_labels_encoded = np.array([])\n",
    "    all_original_valid_labels_encoded = np.array([])\n",
    "else:\n",
    "    label_encoder = LabelEncoder() #\n",
    "    all_original_train_labels_encoded = label_encoder.fit_transform(all_original_train_labels_str) #\n",
    "    if all_original_valid_labels_str: # Only transform if validation labels exist\n",
    "         all_original_valid_labels_encoded = label_encoder.transform(all_original_valid_labels_str) #\n",
    "    else:\n",
    "        all_original_valid_labels_encoded = np.array([], dtype=int) # Ensure it's an empty array of appropriate type\n",
    "        print(\"Warning: No original validation labels found to encode.\")\n",
    "\n",
    "    label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)} #\n",
    "    print(\"\\nLabel mapping (numerical_label: species_name):\") #\n",
    "    for i in range(min(5, len(label_mapping))): # Print first 5\n",
    "        print(f\"{i}: {label_mapping[i]}\")\n",
    "    if len(label_mapping) > 5: print(\"...\") #\n",
    "    \n",
    "    # Update NUM_CLASSES if label_encoder found a different number of classes than initially set\n",
    "    if len(label_encoder.classes_) != NUM_CLASSES and len(label_encoder.classes_) > 0:\n",
    "        print(f\"Warning: Number of classes from LabelEncoder ({len(label_encoder.classes_)}) differs from NUM_CLASSES ({NUM_CLASSES}). Updating NUM_CLASSES to {len(label_encoder.classes_)}.\")\n",
    "        NUM_CLASSES = len(label_encoder.classes_)\n",
    "    elif len(label_encoder.classes_) == 0 :\n",
    "        print(\"ERROR: LabelEncoder found 0 classes. Dataset might be empty or incorrectly structured.\")\n",
    "        NUM_CLASSES = 0\n",
    "\n",
    "# --- DEĞİŞİKLİKLER BURADA BAŞLIYOR ---\n",
    "# Define the number of samples per class for each set\n",
    "SAMPLES_PER_CLASS_TRAIN = 400\n",
    "SAMPLES_PER_CLASS_TEST = 80\n",
    "SAMPLES_PER_CLASS_VALID = 80\n",
    "\n",
    "# Training set paths and labels\n",
    "X_train_paths_temp = []\n",
    "y_train_labels_encoded_list_temp = []\n",
    "original_train_paths_np = np.array(all_original_train_paths)\n",
    "original_train_labels_np = np.array(all_original_train_labels_encoded)\n",
    "\n",
    "for class_idx in range(NUM_CLASSES): #\n",
    "    class_paths = original_train_paths_np[original_train_labels_np == class_idx]\n",
    "    if len(class_paths) == 0:\n",
    "        print(f\"Warning: No original training images found for class index {class_idx} ({label_mapping.get(class_idx, 'Unknown')}).\")\n",
    "        continue\n",
    "    \n",
    "    random.shuffle(class_paths) # Shuffle paths for this class\n",
    "    \n",
    "    selected_train_paths = class_paths[:SAMPLES_PER_CLASS_TRAIN]\n",
    "    X_train_paths_temp.extend(selected_train_paths)\n",
    "    y_train_labels_encoded_list_temp.extend([class_idx] * len(selected_train_paths))\n",
    "    if len(selected_train_paths) < SAMPLES_PER_CLASS_TRAIN:\n",
    "        print(f\"Warning: For training class {label_mapping.get(class_idx, 'Unknown')}, only {len(selected_train_paths)} samples found (requested {SAMPLES_PER_CLASS_TRAIN}).\")\n",
    "\n",
    "X_train_paths = X_train_paths_temp\n",
    "y_train_labels_encoded_np = np.array(y_train_labels_encoded_list_temp)\n",
    "\n",
    "\n",
    "# Validation and Test set paths and labels\n",
    "X_val_paths_temp = []\n",
    "y_val_labels_encoded_list_temp = []\n",
    "X_test_paths_temp = []\n",
    "y_test_labels_encoded_list_temp = []\n",
    "\n",
    "if all_original_valid_paths: # Proceed only if there are validation paths\n",
    "    original_valid_paths_np = np.array(all_original_valid_paths) #\n",
    "    original_valid_labels_np = np.array(all_original_valid_labels_encoded) #\n",
    "\n",
    "    for class_idx in range(NUM_CLASSES): # Iterate up to the effective NUM_CLASSES\n",
    "        class_paths = original_valid_paths_np[original_valid_labels_np == class_idx] #\n",
    "        \n",
    "        if len(class_paths) < SAMPLES_PER_CLASS_TEST + SAMPLES_PER_CLASS_VALID:\n",
    "            print(f\"Warning: Not enough original validation images for class {label_mapping.get(class_idx, 'Unknown')} to create test ({SAMPLES_PER_CLASS_TEST}) and validation ({SAMPLES_PER_CLASS_VALID}) sets. Found {len(class_paths)}.\")\n",
    "            # Adjust if needed, e.g., by taking fewer or splitting what's available\n",
    "            # For simplicity here, we'll take what we can, prioritizing test then validation\n",
    "            random.shuffle(class_paths) #\n",
    "            current_class_test_paths = class_paths[:SAMPLES_PER_CLASS_TEST]\n",
    "            current_class_val_paths = class_paths[SAMPLES_PER_CLASS_TEST : SAMPLES_PER_CLASS_TEST + SAMPLES_PER_CLASS_VALID]\n",
    "            \n",
    "            X_test_paths_temp.extend(current_class_test_paths)\n",
    "            y_test_labels_encoded_list_temp.extend([class_idx] * len(current_class_test_paths))\n",
    "            \n",
    "            X_val_paths_temp.extend(current_class_val_paths)\n",
    "            y_val_labels_encoded_list_temp.extend([class_idx] * len(current_class_val_paths))\n",
    "            continue\n",
    "\n",
    "        random.shuffle(class_paths) # Shuffle paths for this class\n",
    "        \n",
    "        # Assign SAMPLES_PER_CLASS_TEST for test set\n",
    "        selected_test_paths = class_paths[:SAMPLES_PER_CLASS_TEST]\n",
    "        X_test_paths_temp.extend(selected_test_paths)\n",
    "        y_test_labels_encoded_list_temp.extend([class_idx] * len(selected_test_paths))\n",
    "        \n",
    "        # Assign SAMPLES_PER_CLASS_VALID for validation set from the remaining\n",
    "        selected_val_paths = class_paths[SAMPLES_PER_CLASS_TEST : SAMPLES_PER_CLASS_TEST + SAMPLES_PER_CLASS_VALID]\n",
    "        X_val_paths_temp.extend(selected_val_paths)\n",
    "        y_val_labels_encoded_list_temp.extend([class_idx] * len(selected_val_paths))\n",
    "else:\n",
    "    print(\"Warning: 'all_original_valid_paths' is empty. Validation and Test sets will be empty.\") #\n",
    "\n",
    "X_val_paths = X_val_paths_temp\n",
    "y_val_labels_encoded_np = np.array(y_val_labels_encoded_list_temp)\n",
    "X_test_paths = X_test_paths_temp\n",
    "y_test_labels_encoded_np = np.array(y_test_labels_encoded_list_temp)\n",
    "\n",
    "# --- DEĞİŞİKLİKLER BURADA BİTİYOR ---\n",
    "\n",
    "print(f\"\\n--- Dataset Split Path Counts (Targeting {SAMPLES_PER_CLASS_TRAIN} Train, {SAMPLES_PER_CLASS_TEST} Test, {SAMPLES_PER_CLASS_VALID} Val per class) ---\") #\n",
    "print(f\"Actual training image paths collected: {len(X_train_paths)}\") #\n",
    "print(f\"Actual test image paths collected: {len(X_test_paths)}\") #\n",
    "print(f\"Actual validation image paths collected: {len(X_val_paths)}\") #\n",
    "\n",
    "# Convert label lists to NumPy arrays (ensure they are arrays even if empty)\n",
    "y_train_labels_encoded_np = np.array(y_train_labels_encoded_np) #\n",
    "y_val_labels_encoded_np = np.array(y_val_labels_encoded_list_temp) # Use temp list before this line\n",
    "y_test_labels_encoded_np = np.array(y_test_labels_encoded_list_temp)# Use temp list before this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0.4: Data Preprocessing (Image Loading, Resizing, and Storage in Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3: Efficient Image Loading, Resizing, and Storage (BGR format in Memory)\n",
    "# This step reads all images from the split paths ONCE, resizes them, \n",
    "# and stores them in memory as BGR NumPy arrays. \n",
    "# It also filters labels for images that couldn't be loaded/processed robustly.\n",
    "# Normalization (e.g., to [0,1]) will be applied later if a specific model requires it.\n",
    "# For traditional feature extractors (Part 1), we'll often use the 0-255 BGR or Grayscale images derived from these.\n",
    "\n",
    "# %% Step 3 Code\n",
    "print(\"--- Loading and Resizing All Images (BGR format) into Memory & Filtering Labels ---\")\n",
    "\n",
    "def load_resize_and_filter_bgr_efficiently(image_paths, original_labels_np, target_width, target_height, max_retries=3, retry_delay_seconds=1):\n",
    "    \"\"\"\n",
    "    Loads images from paths, resizes to target_width x target_height, stores as BGR.\n",
    "    Retries reading an image if it fails, up to max_retries using PIL for robustness.\n",
    "    Filters out images that cannot be loaded/processed and their corresponding labels.\n",
    "    Returns NumPy arrays of loaded BGR images, filtered labels, and successfully loaded paths.\n",
    "    \"\"\"\n",
    "    loaded_images_bgr_list = []\n",
    "    filtered_labels_list = []\n",
    "    successfully_loaded_paths_list = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    if not image_paths: # Handle empty image_paths list\n",
    "        print(\"Warning: Input image_paths list is empty for efficient loading.\")\n",
    "        # Return empty arrays with appropriate shapes if possible, or just empty arrays\n",
    "        return np.empty((0, target_height, target_width, IMG_CHANNELS), dtype=np.uint8), \\\n",
    "               np.array([], dtype=original_labels_np.dtype if original_labels_np.size > 0 else int), \\\n",
    "               []\n",
    "\n",
    "    total_paths = len(image_paths)\n",
    "    print(f\"Attempting to load and resize {total_paths} images to ({target_width}x{target_height})...\")\n",
    "    \n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        img_bgr = None\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                pil_img = Image.open(img_path)\n",
    "                pil_img_rgb = pil_img.convert('RGB') \n",
    "                img_bgr = cv2.cvtColor(np.array(pil_img_rgb), cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                if img_bgr is not None:\n",
    "                    break \n",
    "            except FileNotFoundError:\n",
    "                print(f\"ERROR (Attempt {attempt+1}/{max_retries}): File not found {img_path}. Skipping this image.\")\n",
    "                img_bgr = None \n",
    "                break \n",
    "            except Exception as e_read:\n",
    "                print(f\"Warning (Attempt {attempt+1}/{max_retries}): Error reading/converting image {img_path}: {e_read}. Retrying in {retry_delay_seconds}s...\")\n",
    "                time.sleep(retry_delay_seconds)\n",
    "        \n",
    "        if img_bgr is None:\n",
    "            print(f\"ERROR: Failed to load/convert image {img_path} after {max_retries} attempts, skipping.\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Ensure image has 3 channels after conversion\n",
    "            if len(img_bgr.shape) != 3 or img_bgr.shape[2] != 3:\n",
    "                print(f\"Warning: Image {img_path} does not have 3 channels after conversion (shape: {img_bgr.shape}), attempting to force BGR.\")\n",
    "                if len(img_bgr.shape) == 2: \n",
    "                    img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)\n",
    "                elif img_bgr.shape[2] == 1: \n",
    "                     img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)\n",
    "                elif img_bgr.shape[2] == 4: \n",
    "                    img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_BGRA2BGR)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported number of channels: {img_bgr.shape[2]}\")\n",
    "\n",
    "            img_bgr_resized = cv2.resize(img_bgr, (target_width, target_height), interpolation=cv2.INTER_AREA)\n",
    "            loaded_images_bgr_list.append(img_bgr_resized)\n",
    "            filtered_labels_list.append(original_labels_np[i])\n",
    "            successfully_loaded_paths_list.append(img_path)\n",
    "        except Exception as e_proc:\n",
    "            print(f\"Error resizing/processing image {img_path} (shape: {img_bgr.shape if img_bgr is not None else 'None'}, dtype: {img_bgr.dtype if img_bgr is not None else 'None'}): {e_proc}, skipping.\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        if (i + 1) % 250 == 0 or (i + 1) == total_paths: \n",
    "            print(f\"  Processed {i+1}/{total_paths} image paths for this set.\")\n",
    "            \n",
    "    print(f\"Finished loading for this set. Successfully loaded/resized {len(loaded_images_bgr_list)} images. Skipped {skipped_count} images.\")\n",
    "    \n",
    "    # Convert lists to NumPy arrays, ensuring correct dtype and shape for empty lists\n",
    "    final_images_array = np.array(loaded_images_bgr_list, dtype=np.uint8) if loaded_images_bgr_list else np.empty((0, target_height, target_width, IMG_CHANNELS), dtype=np.uint8)\n",
    "    final_labels_array = np.array(filtered_labels_list, dtype=original_labels_np.dtype if original_labels_np.size > 0 else int) if filtered_labels_list else np.array([], dtype=original_labels_np.dtype if original_labels_np.size > 0 else int)\n",
    "    \n",
    "    return final_images_array, final_labels_array, successfully_loaded_paths_list\n",
    "\n",
    "# --- Load all images into memory. These will be used by subsequent parts. ---\n",
    "# The *_final variables will hold the actual BGR image data (0-255 range) and their filtered labels.\n",
    "# X_train_paths etc. are from Step 2 Code cell\n",
    "\n",
    "if 'X_train_paths' not in globals() or not X_train_paths:\n",
    "    print(\"ERROR: X_train_paths is not defined or empty. Please run Step 1 and Step 2 first.\")\n",
    "    # Initialize to prevent errors if subsequent cells are run accidentally\n",
    "    X_train_images_bgr, y_train_final, X_train_paths_final = np.array([]), np.array([]), []\n",
    "    X_val_images_bgr, y_val_final, X_val_paths_final = np.array([]), np.array([]), []\n",
    "    X_test_images_bgr, y_test_final, X_test_paths_final = np.array([]), np.array([]), []\n",
    "else:\n",
    "    X_train_images_bgr, y_train_final, X_train_paths_final = load_resize_and_filter_bgr_efficiently(X_train_paths, y_train_labels_encoded_np, IMG_WIDTH, IMG_HEIGHT)\n",
    "    X_val_images_bgr, y_val_final, X_val_paths_final = load_resize_and_filter_bgr_efficiently(X_val_paths, y_val_labels_encoded_np, IMG_WIDTH, IMG_HEIGHT)\n",
    "    X_test_images_bgr, y_test_final, X_test_paths_final = load_resize_and_filter_bgr_efficiently(X_test_paths, y_test_labels_encoded_np, IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "print(\"\\n--- Final Data Shapes After Loading Images into Memory ---\")\n",
    "print(f\"X_train_images_bgr shape: {X_train_images_bgr.shape}, y_train_final shape: {y_train_final.shape}\")\n",
    "print(f\"X_val_images_bgr shape: {X_val_images_bgr.shape}, y_val_final shape: {y_val_final.shape}\")\n",
    "print(f\"X_test_images_bgr shape: {X_test_images_bgr.shape}, y_test_final shape: {y_test_final.shape}\")\n",
    "\n",
    "# Update NUM_CLASSES and target_names_part1 based on actual unique labels found AFTER filtering\n",
    "# This is crucial if some classes were entirely skipped due to loading errors.\n",
    "if y_train_final.size > 0:\n",
    "    # Concatenate all filtered labels to find the true set of classes present in the loaded data\n",
    "    all_loaded_labels_list = []\n",
    "    if y_train_final.size > 0: all_loaded_labels_list.append(y_train_final)\n",
    "    if y_val_final.size > 0: all_loaded_labels_list.append(y_val_final)\n",
    "    if y_test_final.size > 0: all_loaded_labels_list.append(y_test_final)\n",
    "    \n",
    "    if all_loaded_labels_list: # If any labels exist after filtering\n",
    "        all_loaded_labels = np.concatenate(all_loaded_labels_list, axis=0)\n",
    "        unique_loaded_labels = np.unique(all_loaded_labels)\n",
    "        actual_num_classes_loaded = len(unique_loaded_labels)\n",
    "    else: # No labels loaded at all\n",
    "        actual_num_classes_loaded = 0\n",
    "        unique_loaded_labels = np.array([])\n",
    "        print(\"CRITICAL WARNING: No labels loaded into y_train_final, y_val_final, or y_test_final. Dataset might be empty or all images failed to load.\")\n",
    "\n",
    "    if actual_num_classes_loaded != NUM_CLASSES:\n",
    "        print(f\"INFO: Number of unique labels in all loaded data ({actual_num_classes_loaded}) \"\n",
    "              f\"differs from initial NUM_CLASSES ({NUM_CLASSES}). Updating NUM_CLASSES to {actual_num_classes_loaded}.\")\n",
    "        NUM_CLASSES = actual_num_classes_loaded\n",
    "    \n",
    "    if 'label_mapping' in globals():\n",
    "        # Create target names based on labels that are actually present and in label_mapping\n",
    "        target_names_part1 = [label_mapping.get(i, str(i)) for i in sorted(list(unique_loaded_labels))]\n",
    "        if len(target_names_part1) != actual_num_classes_loaded and actual_num_classes_loaded > 0:\n",
    "             print(f\"Warning: Mismatch in target_names_part1 generation ({len(target_names_part1)}) and actual_num_classes_loaded ({actual_num_classes_loaded}). Some labels might not be in label_mapping. Using sorted unique labels as strings for missing ones.\")\n",
    "             # This line ensures target_names_part1 has the correct length, using string of label if not in mapping\n",
    "             target_names_part1 = [label_mapping.get(i, str(i)) for i in sorted(list(unique_loaded_labels))]\n",
    "    else: # Fallback if label_mapping is not defined\n",
    "        target_names_part1 = [str(i) for i in sorted(list(unique_loaded_labels))]\n",
    "        if actual_num_classes_loaded > 0 : print(\"Warning: label_mapping not found. Using sorted unique numerical labels for classification report target names.\")\n",
    "        else: print(\"Warning: label_mapping not found and no labels loaded to derive target_names.\")\n",
    "\n",
    "    print(f\"Effective NUM_CLASSES for reports: {NUM_CLASSES}\")\n",
    "    print(f\"Target names for reports (first 5 if available): {target_names_part1[:5] if target_names_part1 else 'N/A'}\")\n",
    "\n",
    "else: # Handle case where y_train_final itself is empty (meaning no training images loaded)\n",
    "    print(\"ERROR: y_train_final is empty after loading. Cannot reliably set NUM_CLASSES or target_names_part1. This indicates a major issue with training data loading.\")\n",
    "    NUM_CLASSES = 0 # Set to 0 if no training data, to prevent errors in later cells expecting NUM_CLASSES\n",
    "    target_names_part1 = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Part 1: Classification According to Feature Extraction (HSV, HoG, Gabor - Parallel with MLP)\n",
    "\n",
    "# %%\n",
    "# Önceki importlarınıza ek olarak veya mevcut olanları kontrol ederek:\n",
    "from skimage.feature import hog\n",
    "from skimage import color, filters # Gabor için filters modülü\n",
    "import cv2 # OpenCV zaten import edilmiş olmalı\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "\n",
    "# ML Modelleri ve Pipeline için gerekli importlar\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier # Naive Bayes yerine MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Part 0'dan gelen değişkenler varsayılıyor:\n",
    "# X_train_images_bgr, y_train_final\n",
    "# X_test_images_bgr, y_test_final\n",
    "# target_names_part1 (sınıf isimleri listesi)\n",
    "# IMG_WIDTH, IMG_HEIGHT (örneğin 128, 128)\n",
    "\n",
    "# --- Özellik Çıkarım Fonksiyonları ---\n",
    "# (extract_hsv_histogram_single_image, extract_hog_features_single_image, extract_gabor_features_single_image fonksiyonları\n",
    "# bir önceki yanıttaki gibi burada tanımlı olmalıdır)\n",
    "\n",
    "def extract_hsv_histogram_single_image(image_bgr, h_bins=8, s_bins=4, v_bins=4):\n",
    "    \"\"\"Tek bir BGR resimden HSV renk histogramı özelliklerini çıkarır.\"\"\"\n",
    "    if image_bgr is None:\n",
    "        return None\n",
    "    try:\n",
    "        image_hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
    "        hist = cv2.calcHist([image_hsv], [0, 1, 2], None, \n",
    "                            [h_bins, s_bins, v_bins], \n",
    "                            [0, 180, 0, 256, 0, 256]) # Hue için 0-179 aralığı\n",
    "        cv2.normalize(hist, hist) # Normalizasyon\n",
    "        return hist.flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting HSV histogram: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_hog_features_single_image(image_bgr):\n",
    "    \"\"\"Tek bir BGR resimden HoG özelliklerini çıkarır.\"\"\"\n",
    "    if image_bgr is None:\n",
    "        return None\n",
    "    try:\n",
    "        image_gray = color.rgb2gray(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
    "        features = hog(image_gray, pixels_per_cell=(16, 16), \n",
    "                       cells_per_block=(2, 2), orientations=9,\n",
    "                       visualize=False, feature_vector=True, block_norm='L2-Hys')\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting HoG features: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_gabor_features_single_image(image_bgr, num_orientations=8, frequencies=(0.05, 0.25, 0.5), sigmas=(1,3)):\n",
    "    \"\"\"Tek bir BGR resimden Gabor filtresi tepkilerinin ortalama ve std. sapmasını çıkarır.\"\"\"\n",
    "    if image_bgr is None:\n",
    "        return None\n",
    "    try:\n",
    "        image_gray = color.rgb2gray(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
    "        gabor_features = []\n",
    "        for theta_idx in range(num_orientations):\n",
    "            theta = theta_idx / float(num_orientations) * np.pi\n",
    "            for frequency in frequencies:\n",
    "                for sigma_val in sigmas:\n",
    "                    filt_real, filt_imag = filters.gabor(image_gray, frequency=frequency, theta=theta, sigma_x=sigma_val, sigma_y=sigma_val)\n",
    "                    magnitude = np.sqrt(filt_real**2 + filt_imag**2)\n",
    "                    gabor_features.append(np.mean(magnitude))\n",
    "                    gabor_features.append(np.std(magnitude))\n",
    "        return np.array(gabor_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting Gabor features: {e}\")\n",
    "        return None\n",
    "\n",
    "# %%\n",
    "# --- ML Modellerini Getiren Fonksiyon (NaiveBayes yerine MLP ile GÜNCELLENDİ) ---\n",
    "def get_ml_models():\n",
    "    models = {\n",
    "        \"SVM\": SVC(kernel='rbf', C=1.0, random_state=42, probability=True, max_iter=1000),\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        \"LogisticRegression\": LogisticRegression(solver='liblinear', max_iter=200, random_state=42), \n",
    "        \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42, early_stopping=True) # NaiveBayes yerine\n",
    "    }\n",
    "    return models\n",
    "\n",
    "# %%\n",
    "# --- Her bir işlem (thread/process) için ana işleyici fonksiyon ---\n",
    "# process_feature_and_models fonksiyonu bir önceki yanıttaki (Turn 13 veya 14) gibi kalacak.\n",
    "# Sadece get_ml_models() fonksiyonunun güncellenmiş halini kullanacak.\n",
    "# Bu fonksiyonu bir önceki yanıttan kopyalayıp buraya veya bir önceki hücreye ekleyebilirsiniz.\n",
    "# Eğer zaten notebook'unuzda varsa, sadece get_ml_models çağrısının doğru olduğundan emin olun.\n",
    "# (Örnek: Bir önceki yanıttaki gibi process_feature_and_models fonksiyonu)\n",
    "\n",
    "def process_feature_and_models(feature_name, feature_extractor_func,\n",
    "                               X_train_imgs, y_train_labels,\n",
    "                               X_test_imgs, y_test_labels,\n",
    "                               ml_models_dict, target_names):\n",
    "    \"\"\"\n",
    "    Belirli bir özellik çıkarıcıyı kullanarak özellikleri çıkarır,\n",
    "    ardından verilen ML modellerini eğitir ve test eder.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting process for {feature_name} ---\")\n",
    "    \n",
    "    # 1. Özellik Çıkarımı\n",
    "    print(f\"Extracting {feature_name} features for training set...\")\n",
    "    start_fe_time = time.time()\n",
    "    # Özellik çıkarımını doğrudan numpy array üzerinde yapalım\n",
    "    X_train_features_list = [feature_extractor_func(img) for img in X_train_imgs]\n",
    "    \n",
    "    # Başarısız olanları (None dönenleri) ve karşılık gelen etiketleri filtrele\n",
    "    successful_train_indices = [i for i, f in enumerate(X_train_features_list) if f is not None]\n",
    "    if len(successful_train_indices) == 0:\n",
    "        print(f\"ERROR: No training features successfully extracted for {feature_name}. Skipping.\")\n",
    "        return {feature_name: \"Feature extraction failed for all training samples.\"}\n",
    "    X_train_features = np.array([X_train_features_list[i] for i in successful_train_indices])\n",
    "    y_train_labels_filtered = y_train_labels[successful_train_indices]\n",
    "\n",
    "    if X_train_features.size == 0:\n",
    "        print(f\"ERROR: No training features extracted (after filtering None) for {feature_name}. Skipping.\")\n",
    "        return {feature_name: \"Feature extraction resulted in empty training set.\"}\n",
    "\n",
    "    print(f\"Extracting {feature_name} features for test set...\")\n",
    "    X_test_features_list = [feature_extractor_func(img) for img in X_test_imgs]\n",
    "    \n",
    "    successful_test_indices = [i for i, f in enumerate(X_test_features_list) if f is not None]\n",
    "    if len(successful_test_indices) == 0:\n",
    "        print(f\"ERROR: No test features successfully extracted for {feature_name}. Skipping model evaluations for this feature.\")\n",
    "        return {feature_name: \"Feature extraction failed for all test samples.\"}\n",
    "    X_test_features = np.array([X_test_features_list[i] for i in successful_test_indices])\n",
    "    y_test_labels_filtered = y_test_labels[successful_test_indices]\n",
    "    \n",
    "    if X_test_features.size == 0:\n",
    "        print(f\"ERROR: No test features extracted (after filtering None) for {feature_name}. Skipping.\")\n",
    "        return {feature_name: \"Feature extraction resulted in empty test set.\"}\n",
    "\n",
    "    end_fe_time = time.time()\n",
    "    print(f\"{feature_name} feature extraction completed in {end_fe_time - start_fe_time:.2f} seconds.\")\n",
    "    print(f\"Train features shape: {X_train_features.shape}, Test features shape: {X_test_features.shape}\")\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    # 2. ML Modellerini Eğitme ve Test Etme\n",
    "    for model_name, model_instance in ml_models_dict.items():\n",
    "        print(f\"\\nTraining {model_name} with {feature_name} features...\")\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()), \n",
    "            ('classifier', model_instance)\n",
    "        ])\n",
    "        \n",
    "        start_train_time = time.time()\n",
    "        try:\n",
    "            if X_train_features.shape[1] != X_test_features.shape[1]:\n",
    "                 print(f\"ERROR: Feature dimension mismatch for {feature_name} between train ({X_train_features.shape[1]}) and test ({X_test_features.shape[1]}). Skipping {model_name}.\")\n",
    "                 results[f\"{feature_name}_{model_name}\"] = {\"error\": \"Feature dimension mismatch\"}\n",
    "                 continue\n",
    "\n",
    "            if X_train_features.shape[0] == 0 or y_train_labels_filtered.shape[0] == 0:\n",
    "                print(f\"ERROR: Empty training features or labels for {feature_name}. Skipping {model_name}.\")\n",
    "                results[f\"{feature_name}_{model_name}\"] = {\"error\": \"Empty training data\"}\n",
    "                continue\n",
    "            \n",
    "            if len(np.unique(y_train_labels_filtered)) < 2 :\n",
    "                print(f\"ERROR: Training data for {feature_name} has less than 2 classes. Skipping {model_name}.\")\n",
    "                results[f\"{feature_name}_{model_name}\"] = {\"error\": \"Less than 2 classes in training data\"}\n",
    "                continue\n",
    "\n",
    "\n",
    "            pipeline.fit(X_train_features, y_train_labels_filtered)\n",
    "        except ValueError as e:\n",
    "            print(f\"ERROR training {model_name} with {feature_name}: {e}. Skipping this model.\")\n",
    "            results[f\"{feature_name}_{model_name}\"] = {\"error\": str(e)}\n",
    "            continue\n",
    "        end_train_time = time.time()\n",
    "        print(f\"{model_name} training completed in {end_train_time - start_train_time:.2f} seconds.\")\n",
    "        \n",
    "        # Test seti üzerinde değerlendirme\n",
    "        if X_test_features.shape[0] > 0 and y_test_labels_filtered.shape[0] > 0 :\n",
    "            if len(np.unique(y_test_labels_filtered)) < 2 and len(np.unique(y_test_labels_filtered)) != len(np.unique(y_train_labels_filtered)):\n",
    "                 print(f\"Warning: Test data for {feature_name} has less than 2 classes or different class set than train. Report might be problematic.\")\n",
    "            \n",
    "            y_pred = pipeline.predict(X_test_features)\n",
    "            accuracy = accuracy_score(y_test_labels_filtered, y_pred)\n",
    "            try:\n",
    "                # Sınıflandırma raporu için etiketlerin hem train hem de test setinde olmasını sağlamak gerekebilir\n",
    "                # Eğer target_names verilmiyorsa, sadece mevcut etiketleri kullanır.\n",
    "                # Modelin eğitildiği sınıflarla testteki sınıflar aynı olmalı.\n",
    "                unique_labels_in_test_and_train = sorted(list(set(y_train_labels_filtered) | set(y_test_labels_filtered)))\n",
    "                current_target_names = [target_names[i] for i in unique_labels_in_test_and_train if i < len(target_names)]\n",
    "\n",
    "                report = classification_report(y_test_labels_filtered, y_pred, labels=unique_labels_in_test_and_train, target_names=current_target_names, zero_division=0, output_dict=True)\n",
    "            except ValueError as e: \n",
    "                print(f\"Warning: Could not generate classification report for {model_name} with {feature_name} due to label mismatch or other issues: {e}\")\n",
    "                report = {\"error\": str(e), \"accuracy_manual\": accuracy}\n",
    "\n",
    "\n",
    "            print(f\"\\n{model_name} with {feature_name} - Test Set:\")\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "            if \"error\" not in report and 'macro avg' in report:\n",
    "                print(f\"Macro Avg F1-score: {report.get('macro avg', {}).get('f1-score', 'N/A'):.4f}\")\n",
    "            \n",
    "            results[f\"{feature_name}_{model_name}\"] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"classification_report_dict\": report \n",
    "            }\n",
    "        else:\n",
    "            print(f\"Test features for {feature_name} are empty or labels are missing. Skipping evaluation for {model_name}.\")\n",
    "            results[f\"{feature_name}_{model_name}\"] = {\"error\": \"Empty test features or labels\"}\n",
    "            \n",
    "    print(f\"--- Finished process for {feature_name} ---\")\n",
    "    return {feature_name: results}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# %%\n",
    "# --- Helper function for chunked Gabor processing ---\n",
    "def extract_gabor_features_chunk(args):\n",
    "    \"\"\"\n",
    "    Extract Gabor features for a chunk of images\n",
    "    Args: (image_chunk, start_idx, chunk_id, feature_extractor_func)\n",
    "    Returns: (chunk_id, start_idx, features_list)\n",
    "    \"\"\"\n",
    "    image_chunk, start_idx, chunk_id, feature_extractor_func = args\n",
    "    print(f\"Processing Gabor chunk {chunk_id + 1} (images {start_idx} to {start_idx + len(image_chunk) - 1})\")\n",
    "    \n",
    "    features_list = []\n",
    "    for img in image_chunk:\n",
    "        features = feature_extractor_func(img)\n",
    "        features_list.append(features)\n",
    "    \n",
    "    print(f\"Gabor chunk {chunk_id + 1} completed\")\n",
    "    return chunk_id, start_idx, features_list\n",
    "\n",
    "# %%\n",
    "# --- Phase 1: Feature Extraction Functions ---\n",
    "def extract_features_single_core(feature_name, feature_extractor_func, X_images, dataset_type=\"train\"):\n",
    "    \"\"\"\n",
    "    Extract features using single core (for HSV and HoG)\n",
    "    \"\"\"\n",
    "    print(f\"Extracting {feature_name} features for {dataset_type} set (single core)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    features_list = [feature_extractor_func(img) for img in X_images]\n",
    "    \n",
    "    # Filter out failed extractions (None values)\n",
    "    successful_indices = [i for i, f in enumerate(features_list) if f is not None]\n",
    "    \n",
    "    if len(successful_indices) == 0:\n",
    "        print(f\"ERROR: No {dataset_type} features successfully extracted for {feature_name}\")\n",
    "        return feature_name, dataset_type, None, None\n",
    "    \n",
    "    features_array = np.array([features_list[i] for i in successful_indices])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"{feature_name} {dataset_type} feature extraction completed in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"{dataset_type.capitalize()} features shape: {features_array.shape}\")\n",
    "    \n",
    "    return feature_name, dataset_type, features_array, successful_indices\n",
    "\n",
    "def extract_gabor_features_multicore(X_images, dataset_type=\"train\", num_cores=10):\n",
    "    \"\"\"\n",
    "    Extract Gabor features using multiple cores by splitting the dataset\n",
    "    \"\"\"\n",
    "    print(f\"Extracting Gabor features for {dataset_type} set using {num_cores} cores...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Split images into chunks for parallel processing\n",
    "    total_images = len(X_images)\n",
    "    chunk_size = max(1, total_images // num_cores)\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, total_images, chunk_size):\n",
    "        end_idx = min(i + chunk_size, total_images)\n",
    "        image_chunk = X_images[i:end_idx]\n",
    "        chunks.append((image_chunk, i, len(chunks), extract_gabor_features_single_image))\n",
    "    \n",
    "    print(f\"Split {total_images} images into {len(chunks)} chunks of ~{chunk_size} images each\")\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    try:\n",
    "        with Pool(processes=num_cores) as pool:\n",
    "            chunk_results = pool.map(extract_gabor_features_chunk, chunks)\n",
    "        \n",
    "        # Combine results from all chunks\n",
    "        # Sort by chunk_id to maintain order\n",
    "        chunk_results.sort(key=lambda x: x[0])\n",
    "        \n",
    "        combined_features_list = []\n",
    "        for chunk_id, start_idx, features_list in chunk_results:\n",
    "            combined_features_list.extend(features_list)\n",
    "        \n",
    "        # Filter out failed extractions (None values)\n",
    "        successful_indices = [i for i, f in enumerate(combined_features_list) if f is not None]\n",
    "        \n",
    "        if len(successful_indices) == 0:\n",
    "            print(f\"ERROR: No {dataset_type} Gabor features successfully extracted\")\n",
    "            return \"Gabor\", dataset_type, None, None\n",
    "        \n",
    "        features_array = np.array([combined_features_list[i] for i in successful_indices])\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Gabor {dataset_type} feature extraction completed in {end_time - start_time:.2f} seconds.\")\n",
    "        print(f\"{dataset_type.capitalize()} features shape: {features_array.shape}\")\n",
    "        \n",
    "        return \"Gabor\", dataset_type, features_array, successful_indices\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gabor feature extraction: {e}\")\n",
    "        return \"Gabor\", dataset_type, None, None\n",
    "\n",
    "# %%\n",
    "# --- Phase 2: Parallel Model Training and Evaluation ---\n",
    "def train_and_evaluate_model(args):\n",
    "    \"\"\"\n",
    "    Train and evaluate a single model with given features\n",
    "    Args: (feature_name, model_name, model_instance, X_train_features, y_train_filtered, \n",
    "           X_test_features, y_test_filtered, target_names)\n",
    "    \"\"\"\n",
    "    (feature_name, model_name, model_instance, X_train_features, y_train_filtered,\n",
    "     X_test_features, y_test_filtered, target_names) = args\n",
    "    \n",
    "    print(f\"Training {model_name} with {feature_name} features...\")\n",
    "    \n",
    "    # Create pipeline with scaling\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('classifier', model_instance)\n",
    "    ])\n",
    "    \n",
    "    start_train_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Validation checks\n",
    "        if X_train_features.shape[1] != X_test_features.shape[1]:\n",
    "            return (feature_name, model_name, {\"error\": \"Feature dimension mismatch\"})\n",
    "        \n",
    "        if X_train_features.shape[0] == 0 or y_train_filtered.shape[0] == 0:\n",
    "            return (feature_name, model_name, {\"error\": \"Empty training data\"})\n",
    "        \n",
    "        if len(np.unique(y_train_filtered)) < 2:\n",
    "            return (feature_name, model_name, {\"error\": \"Less than 2 classes in training data\"})\n",
    "        \n",
    "        # Train the model\n",
    "        pipeline.fit(X_train_features, y_train_filtered)\n",
    "        \n",
    "        end_train_time = time.time()\n",
    "        print(f\"{model_name} with {feature_name} training completed in {end_train_time - start_train_time:.2f} seconds.\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        if X_test_features.shape[0] > 0 and y_test_filtered.shape[0] > 0:\n",
    "            y_pred = pipeline.predict(X_test_features)\n",
    "            accuracy = accuracy_score(y_test_filtered, y_pred)\n",
    "            \n",
    "            try:\n",
    "                # Generate classification report\n",
    "                unique_labels = sorted(list(set(y_train_filtered) | set(y_test_filtered)))\n",
    "                current_target_names = [target_names[i] for i in unique_labels if i < len(target_names)]\n",
    "                \n",
    "                report = classification_report(\n",
    "                    y_test_filtered, y_pred, \n",
    "                    labels=unique_labels, \n",
    "                    target_names=current_target_names, \n",
    "                    zero_division=0, \n",
    "                    output_dict=True\n",
    "                )\n",
    "                \n",
    "                print(f\"{model_name} with {feature_name} - Accuracy: {accuracy:.4f}\")\n",
    "                if 'macro avg' in report:\n",
    "                    print(f\"{model_name} with {feature_name} - Macro F1: {report['macro avg']['f1-score']:.4f}\")\n",
    "                \n",
    "                return (feature_name, model_name, {\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"classification_report_dict\": report\n",
    "                })\n",
    "                \n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: Could not generate classification report for {model_name} with {feature_name}: {e}\")\n",
    "                return (feature_name, model_name, {\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"classification_report_dict\": {\"error\": str(e), \"accuracy_manual\": accuracy}\n",
    "                })\n",
    "        else:\n",
    "            return (feature_name, model_name, {\"error\": \"Empty test features or labels\"})\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR training {model_name} with {feature_name}: {e}\")\n",
    "        return (feature_name, model_name, {\"error\": str(e)})\n",
    "\n",
    "# %%\n",
    "# --- Main Execution Code ---\n",
    "# Feature extractors and ML models\n",
    "feature_extractors_to_test = {\n",
    "    \"HSV_Hist\": extract_hsv_histogram_single_image,\n",
    "    \"HoG\": extract_hog_features_single_image,\n",
    "    \"Gabor\": extract_gabor_features_single_image\n",
    "}\n",
    "\n",
    "ml_models = get_ml_models()  # MLP included\n",
    "\n",
    "# Check if data is available\n",
    "if not all(hasattr(arr, 'size') and arr.size > 0 for arr in [X_train_images_bgr, y_train_final, X_test_images_bgr, y_test_final]):\n",
    "    print(\"One or more data arrays are empty or not initialized. Skipping Part 1.\")\n",
    "    part1_results = {}\n",
    "else:\n",
    "    print(f\"Starting optimized parallel processing with {cpu_count()} cores available...\")\n",
    "    print(\"Core allocation: HSV (1 core), HoG (1 core), Gabor (10 cores)\")\n",
    "    \n",
    "    # ===== PHASE 1: PARALLEL FEATURE EXTRACTION =====\n",
    "    print(\"\\n=== PHASE 1: FEATURE EXTRACTION ===\")\n",
    "    \n",
    "    extracted_features = {}\n",
    "    successful_indices = {}\n",
    "    \n",
    "    try:\n",
    "        # Start HSV and HoG feature extraction in parallel (2 cores total)\n",
    "        with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "            # Submit HSV tasks\n",
    "            hsv_train_future = executor.submit(\n",
    "                extract_features_single_core, \"HSV_Hist\", \n",
    "                extract_hsv_histogram_single_image, X_train_images_bgr, \"train\"\n",
    "            )\n",
    "            hsv_test_future = executor.submit(\n",
    "                extract_features_single_core, \"HSV_Hist\", \n",
    "                extract_hsv_histogram_single_image, X_test_images_bgr, \"test\"\n",
    "            )\n",
    "            \n",
    "            # Submit HoG tasks\n",
    "            hog_train_future = executor.submit(\n",
    "                extract_features_single_core, \"HoG\", \n",
    "                extract_hog_features_single_image, X_train_images_bgr, \"train\"\n",
    "            )\n",
    "            hog_test_future = executor.submit(\n",
    "                extract_features_single_core, \"HoG\", \n",
    "                extract_hog_features_single_image, X_test_images_bgr, \"test\"\n",
    "            )\n",
    "            \n",
    "            # Process HSV and HoG results\n",
    "            hsv_hog_futures = [hsv_train_future, hsv_test_future, hog_train_future, hog_test_future]\n",
    "            \n",
    "            for future in as_completed(hsv_hog_futures):\n",
    "                feat_name, dataset_type, features_array, indices = future.result()\n",
    "                if features_array is not None:\n",
    "                    extracted_features[f\"{feat_name}_{dataset_type}\"] = features_array\n",
    "                    successful_indices[f\"{feat_name}_{dataset_type}\"] = indices\n",
    "                else:\n",
    "                    print(f\"Failed to extract {feat_name} features for {dataset_type} set\")\n",
    "        \n",
    "        # Extract Gabor features using 10 cores\n",
    "        print(\"\\nStarting Gabor feature extraction with 10 cores...\")\n",
    "        \n",
    "        # Extract Gabor train features\n",
    "        gabor_train_result = extract_gabor_features_multicore(X_train_images_bgr, \"train\", num_cores=10)\n",
    "        feat_name, dataset_type, features_array, indices = gabor_train_result\n",
    "        if features_array is not None:\n",
    "            extracted_features[f\"{feat_name}_{dataset_type}\"] = features_array\n",
    "            successful_indices[f\"{feat_name}_{dataset_type}\"] = indices\n",
    "        else:\n",
    "            print(f\"Failed to extract {feat_name} features for {dataset_type} set\")\n",
    "        \n",
    "        # Extract Gabor test features\n",
    "        gabor_test_result = extract_gabor_features_multicore(X_test_images_bgr, \"test\", num_cores=10)\n",
    "        feat_name, dataset_type, features_array, indices = gabor_test_result\n",
    "        if features_array is not None:\n",
    "            extracted_features[f\"{feat_name}_{dataset_type}\"] = features_array\n",
    "            successful_indices[f\"{feat_name}_{dataset_type}\"] = indices\n",
    "        else:\n",
    "            print(f\"Failed to extract {feat_name} features for {dataset_type} set\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature extraction: {e}\")\n",
    "        extracted_features = {}\n",
    "    \n",
    "    print(f\"\\nFeature extraction completed. Extracted features for: {list(extracted_features.keys())}\")\n",
    "    \n",
    "    # ===== PHASE 2: PARALLEL MODEL TRAINING (12 cores) =====\n",
    "    print(\"\\n=== PHASE 2: MODEL TRAINING AND EVALUATION ===\")\n",
    "    \n",
    "    if extracted_features:\n",
    "        # Prepare training tasks (3 features × 4 models = 12 tasks)\n",
    "        training_tasks = []\n",
    "        \n",
    "        feature_name_mapping = {\n",
    "            \"HSV_Hist\": \"HSV_Hist\",\n",
    "            \"HoG\": \"HoG\", \n",
    "            \"Gabor\": \"Gabor\"\n",
    "        }\n",
    "        \n",
    "        for original_feat_name, mapped_feat_name in feature_name_mapping.items():\n",
    "            train_key = f\"{mapped_feat_name}_train\"\n",
    "            test_key = f\"{mapped_feat_name}_test\"\n",
    "            \n",
    "            if train_key in extracted_features and test_key in extracted_features:\n",
    "                # Get filtered labels based on successful feature extractions\n",
    "                y_train_filtered = y_train_final[successful_indices[train_key]]\n",
    "                y_test_filtered = y_test_final[successful_indices[test_key]]\n",
    "                \n",
    "                # Create training tasks for all models with this feature\n",
    "                for model_name, model_instance in ml_models.items():\n",
    "                    task = (\n",
    "                        mapped_feat_name, model_name, model_instance,\n",
    "                        extracted_features[train_key], y_train_filtered,\n",
    "                        extracted_features[test_key], y_test_filtered,\n",
    "                        target_names_part1\n",
    "                    )\n",
    "                    training_tasks.append(task)\n",
    "        \n",
    "        print(f\"Created {len(training_tasks)} training tasks\")\n",
    "        \n",
    "        # Execute all training tasks in parallel using all 12 cores\n",
    "        part1_results = {}\n",
    "        \n",
    "        if training_tasks:\n",
    "            try:\n",
    "                with Pool(processes=12) as pool:  # Use all 12 cores for model training\n",
    "                    training_results = pool.map(train_and_evaluate_model, training_tasks)\n",
    "                \n",
    "                # Organize results\n",
    "                for feat_name, model_name, result in training_results:\n",
    "                    key = f\"{feat_name}_{model_name}\"\n",
    "                    part1_results[key] = result\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error during model training: {e}\")\n",
    "                part1_results = {}\n",
    "        else:\n",
    "            print(\"No training tasks to execute\")\n",
    "            part1_results = {}\n",
    "    else:\n",
    "        print(\"No features extracted successfully. Skipping model training.\")\n",
    "        part1_results = {}\n",
    "\n",
    "print(\"\\n=== PART 1: ALL PARALLEL PROCESSES COMPLETED ===\")\n",
    "\n",
    "# %%\n",
    "# --- Results Summary ---\n",
    "if part1_results:\n",
    "    print(\"\\n=== RESULTS SUMMARY ===\")\n",
    "    \n",
    "    # Sort results by accuracy for better readability\n",
    "    sorted_results = []\n",
    "    for key, result in part1_results.items():\n",
    "        if isinstance(result, dict) and \"accuracy\" in result:\n",
    "            sorted_results.append((key, result[\"accuracy\"], result))\n",
    "        else:\n",
    "            print(f\"{key}: {result}\")\n",
    "    \n",
    "    # Sort by accuracy (descending)\n",
    "    sorted_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop performing combinations:\")\n",
    "    for key, accuracy, result in sorted_results[:10]:  # Show top 10\n",
    "        feature_name, model_name = key.split('_', 1)\n",
    "        print(f\"{feature_name} + {model_name}: {accuracy:.4f}\")\n",
    "        \n",
    "        if \"classification_report_dict\" in result and \"macro avg\" in result[\"classification_report_dict\"]:\n",
    "            macro_f1 = result[\"classification_report_dict\"][\"macro avg\"][\"f1-score\"]\n",
    "            print(f\"  Macro F1-Score: {macro_f1:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Find best overall combination\n",
    "    if sorted_results:\n",
    "        best_combo, best_accuracy, best_result = sorted_results[0]\n",
    "        best_feature, best_model = best_combo.split('_', 1)\n",
    "        print(f\"🏆 BEST COMBINATION: {best_feature} + {best_model}\")\n",
    "        print(f\"   Accuracy: {best_accuracy:.4f}\")\n",
    "        \n",
    "        if \"classification_report_dict\" in best_result and \"macro avg\" in best_result[\"classification_report_dict\"]:\n",
    "            best_macro_f1 = best_result[\"classification_report_dict\"][\"macro avg\"][\"f1-score\"]\n",
    "            print(f\"   Macro F1-Score: {best_macro_f1:.4f}\")\n",
    "else:\n",
    "    print(\"No results to display.\")\n",
    "\n",
    "print(\"\\nProcessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif # chi2 is an option for non-negative features\n",
    "# Import your ML algorithms from Part 1 (ensure these are the same ones)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Placeholder for your data and models ---\n",
    "# Ensure these variables are correctly loaded/defined from your Part 0 and Part 1\n",
    "# Example:\n",
    "# X_train_dict = {\"color\": train_color_features, \"hog\": train_hog_features, ...}\n",
    "# X_val_dict = {\"color\": val_color_features, \"hog\": val_hog_features, ...} # For tuning k or n_components\n",
    "# X_test_dict = {\"color\": test_color_features, \"hog\": test_hog_features, ...}\n",
    "# y_train, y_val, y_test = your_labels_for_train_val_test_sets\n",
    "\n",
    "# ML algorithms used in Part 1 (use your actual models and their parameters)\n",
    "# ml_algorithms_part1 = {\n",
    "#     \"SVM\": SVC(kernel='linear', C=1, random_state=42),\n",
    "#     \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "#     \"NaiveBayes\": GaussianNB()\n",
    "# }\n",
    "\n",
    "# Dictionary to store results from Part 1 (load or ensure this is accessible)\n",
    "# results_part1 = {\n",
    "#     \"color_SVM\": {\"accuracy\": 0.85, \"precision\": 0.84, ...},\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "# Dictionary to store results for Part 2\n",
    "results_part2 = {}\n",
    "\n",
    "# Helper function to evaluate models\n",
    "def evaluate_and_store_metrics(model, X_train_proc, y_train_labels, X_test_proc, y_test_labels,\n",
    "                               feature_set_name, dim_reduction_method, algo_identifier, results_collection):\n",
    "    print(f\"Training {algo_identifier} on {feature_set_name} features processed by {dim_reduction_method}...\")\n",
    "    model.fit(X_train_proc, y_train_labels)\n",
    "    predictions = model.predict(X_test_proc)\n",
    "\n",
    "    acc = accuracy_score(y_test_labels, predictions)\n",
    "    prec = precision_score(y_test_labels, predictions, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test_labels, predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test_labels, predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    result_key = f\"{feature_set_name}_{dim_reduction_method}_{algo_identifier}\"\n",
    "    results_collection[result_key] = {\n",
    "        \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1_score\": f1\n",
    "    }\n",
    "    print(f\"  Metrics for {result_key}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
    "    return results_collection\n",
    "\n",
    "# Get list of feature types you extracted in Part 1\n",
    "# Make sure X_train_dict, X_val_dict, X_test_dict, y_train, y_val, y_test are defined\n",
    "# And ml_algorithms_part1 dictionary is defined with your models\n",
    "\n",
    "feature_extraction_types = list(X_train_dict.keys()) # e.g., ['color_hist', 'hog', 'sift']\n",
    "\n",
    "for f_type in feature_extraction_types:\n",
    "    print(f\"\\n--- Processing Part 2 for feature type: {f_type} ---\")\n",
    "\n",
    "    X_train_current = X_train_dict[f_type]\n",
    "    # X_val_current = X_val_dict[f_type] # Use for hyperparameter tuning (n_components, k)\n",
    "    X_test_current = X_test_dict[f_type]\n",
    "\n",
    "    # 1. PCA Implementation\n",
    "    print(f\"\\nApplying PCA to '{f_type}' features...\")\n",
    "    scaler_pca = StandardScaler()\n",
    "    X_train_scaled = scaler_pca.fit_transform(X_train_current)\n",
    "    X_test_scaled = scaler_pca.transform(X_test_current)\n",
    "\n",
    "    # Determine n_components for PCA (e.g., explain 95% variance, or fixed number)\n",
    "    # pca_explainer = PCA(random_state=42)\n",
    "    # pca_explainer.fit(X_train_scaled)\n",
    "    # plt.figure()\n",
    "    # plt.plot(np.cumsum(pca_explainer.explained_variance_ratio_))\n",
    "    # plt.xlabel('Number of Components')\n",
    "    # plt.ylabel('Cumulative Explained Variance')\n",
    "    # plt.title(f'PCA Explained Variance for {f_type}')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    # n_pca_components = int(input(f\"Enter n_components for PCA on {f_type} based on plot: \")) # Or set automatically\n",
    "    n_pca_components = 0.95 # Example: Retain 95% of variance\n",
    "\n",
    "    pca_transformer = PCA(n_components=n_pca_components, random_state=42)\n",
    "    X_train_pca_transformed = pca_transformer.fit_transform(X_train_scaled)\n",
    "    X_test_pca_transformed = pca_transformer.transform(X_test_scaled)\n",
    "    print(f\"  Original {f_type} feature dimension: {X_train_scaled.shape[1]}\")\n",
    "    print(f\"  PCA-transformed {f_type} feature dimension: {X_train_pca_transformed.shape[1]}\")\n",
    "\n",
    "    for algo_name, base_model in ml_algorithms_part1.items():\n",
    "        # Re-initialize model to ensure fresh training\n",
    "        current_ml_model = base_model.__class__(**base_model.get_params())\n",
    "        results_part2 = evaluate_and_store_metrics(current_ml_model, X_train_pca_transformed, y_train,\n",
    "                                                 X_test_pca_transformed, y_test,\n",
    "                                                 f_type, \"PCA\", algo_name, results_part2)\n",
    "\n",
    "    # 2. Feature Selection Implementation (e.g., SelectKBest)\n",
    "    print(f\"\\nApplying Feature Selection (SelectKBest) to '{f_type}' features...\")\n",
    "    # Note: Scaling can also be applied before SelectKBest if ML algo is sensitive,\n",
    "    # but f_classif is less dependent on it than PCA. Using X_train_current (unscaled or scaled as per your Part 1).\n",
    "    # If using chi2, ensure features are non-negative. X_train_scaled might have negative values.\n",
    "    # For simplicity, let's use X_train_current. If it contains negative values, f_classif is safer than chi2.\n",
    "    \n",
    "    num_original_feats = X_train_current.shape[1]\n",
    "    if num_original_feats <= 1:\n",
    "        print(f\"  Skipping SelectKBest for {f_type} as it has {num_original_feats} feature(s).\")\n",
    "    else:\n",
    "        # Determine k for SelectKBest (e.g., half the features, or a fixed number like 50/100)\n",
    "        # k_best_features = max(1, min(100, num_original_feats // 2)) # Example\n",
    "        # You might want to tune k using X_val_current\n",
    "        k_best_features = 'all' if num_original_feats < 10 else max(1, num_original_feats // 2) # Adjust k as needed\n",
    "        if k_best_features != 'all' and k_best_features > num_original_feats:\n",
    "             k_best_features = num_original_feats\n",
    "\n",
    "\n",
    "        # Using f_classif as it's generally applicable.\n",
    "        # If your features are strictly non-negative (e.g. histograms), chi2 is also an option.\n",
    "        # Ensure X_train_current is suitable for the score_func (e.g. no negative values for chi2)\n",
    "        # A scaler for feature selection can be added if needed\n",
    "        # scaler_fs = StandardScaler()\n",
    "        # X_train_fs_scaled = scaler_fs.fit_transform(X_train_current)\n",
    "        # X_test_fs_scaled = scaler_fs.transform(X_test_current)\n",
    "\n",
    "        selector = SelectKBest(score_func=f_classif, k=k_best_features)\n",
    "        try:\n",
    "            # Using X_train_current and y_train. If features have negative values and you want to use chi2,\n",
    "            # you'd need to preprocess them (e.g., MinMaxScaler).\n",
    "            X_train_selected_feats = selector.fit_transform(X_train_current, y_train)\n",
    "            X_test_selected_feats = selector.transform(X_test_current)\n",
    "            print(f\"  Original {f_type} feature dimension: {num_original_feats}\")\n",
    "            print(f\"  SelectKBest-transformed {f_type} feature dimension: {X_train_selected_feats.shape[1]}\")\n",
    "\n",
    "            for algo_name, base_model in ml_algorithms_part1.items():\n",
    "                current_ml_model = base_model.__class__(**base_model.get_params())\n",
    "                results_part2 = evaluate_and_store_metrics(current_ml_model, X_train_selected_feats, y_train,\n",
    "                                                         X_test_selected_feats, y_test,\n",
    "                                                         f_type, f\"SelectKBest_k{k_best_features}\", algo_name, results_part2)\n",
    "        except ValueError as e:\n",
    "            print(f\"  Error with SelectKBest for {f_type} (k={k_best_features}): {e}. Check for negative inputs if using chi2, or if k is valid.\")\n",
    "\n",
    "\n",
    "# 3. Comparison of All Results\n",
    "print(\"\\n\\n--- Overall Results Comparison ---\")\n",
    "\n",
    "# Convert Part 1 and Part 2 results to DataFrames for easy viewing\n",
    "results_part1_df = pd.DataFrame.from_dict(results_part1, orient='index')\n",
    "results_part2_df = pd.DataFrame.from_dict(results_part2, orient='index')\n",
    "\n",
    "print(\"\\nResults from Part 1 (Original Features):\")\n",
    "print(results_part1_df)\n",
    "\n",
    "print(\"\\nResults from Part 2 (PCA and Feature Selection):\")\n",
    "print(results_part2_df)\n",
    "\n",
    "# Combine for a comprehensive table\n",
    "all_results_list = []\n",
    "# Part 1\n",
    "for key, metrics in results_part1.items():\n",
    "    parts = key.split('_', 1) # Split only on the first underscore\n",
    "    feature_set = parts[0]\n",
    "    algorithm = parts[1] if len(parts) > 1 else 'N/A'\n",
    "    all_results_list.append({'Feature Set': feature_set, 'Method': 'Original', 'Algorithm': algorithm, **metrics})\n",
    "\n",
    "# Part 2\n",
    "for key, metrics in results_part2.items():\n",
    "    parts = key.split('_', 2) # e.g., \"color_PCA_SVM\" or \"hog_SelectKBest_k50_RF\"\n",
    "    feature_set = parts[0]\n",
    "    method_applied = parts[1]\n",
    "    algorithm = parts[2]\n",
    "    if \"SelectKBest\" in method_applied: # To handle k in the method name if present\n",
    "         method_parts_further = method_applied.split('_') # e.g. SelectKBest_kVal\n",
    "         if len(parts) > 2: # if key was color_SelectKBest_k50_SVM\n",
    "             method_applied = parts[1] # This will be SelectKBest\n",
    "             algorithm = parts[2]\n",
    "         else: # if key was color_SelectKBest_SVM (no k in key from evaluate_and_store_metrics)\n",
    "             algorithm = \"N/A\" # Or adjust parsing based on your exact key format\n",
    "\n",
    "    all_results_list.append({'Feature Set': feature_set, 'Method': method_applied, 'Algorithm': algorithm, **metrics})\n",
    "\n",
    "comparison_df_final = pd.DataFrame(all_results_list)\n",
    "comparison_df_final = comparison_df_final.set_index(['Feature Set', 'Method', 'Algorithm']).sort_index()\n",
    "\n",
    "print(\"\\nComprehensive Comparison Table (Accuracy and F1-Score):\")\n",
    "print(comparison_df_final[['accuracy', 'f1_score']])\n",
    "\n",
    "# --- Add your detailed discussions and interpretations below ---\n",
    "# For each feature type:\n",
    "#   - Compare Original vs. PCA vs. SelectKBest for each ML algorithm.\n",
    "#   - Discuss changes in dimensionality.\n",
    "#   - Impact on performance (accuracy, precision, recall, F1, training time if measured).\n",
    "#   - Why might PCA improve/degrade performance? (decorrelation, noise reduction vs. info loss)\n",
    "#   - Why might Feature Selection improve/degrade performance? (simpler model, reduced overfitting vs. loss of useful info)\n",
    "#   - Compare PCA vs. Feature Selection directly. Which was better and under what conditions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# pca_temp = PCA().fit(X_train_scaled_for_pca)\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(np.cumsum(pca_temp.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.title(f'PCA Explained Variance for {feature_name}')\n",
    "# plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
    "# plt.legend()\n",
    "# plt.grid(True, linestyle=':', alpha=0.7)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3: Applying Feature Selection (SelectKBest) and Evaluating ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% --- Part 2.2: Applying Feature Selection (SelectKBest) and Evaluating ML Models ---\n",
    "print(\"\\n\\n--- Starting Part 2.2: Feature Selection (SelectKBest) ---\")\n",
    "\n",
    "if PART1_FEATURE_TRAIN_DICT_NAME not in globals() or \\\n",
    "   not isinstance(globals()[PART1_FEATURE_TRAIN_DICT_NAME], dict) or \\\n",
    "   'y_train_final' not in globals() or \\\n",
    "   'ml_models' not in globals(): #\n",
    "    print(f\"Error: Part 1 feature sets (e.g., {PART1_FEATURE_TRAIN_DICT_NAME}), labels (y_train_final), or ml_models are not defined/valid. Please run Part 1 first.\")\n",
    "else:\n",
    "    feature_data_train_source_skb = globals()[PART1_FEATURE_TRAIN_DICT_NAME]\n",
    "    feature_data_val_source_skb = globals()[PART1_FEATURE_VAL_DICT_NAME]\n",
    "    feature_data_test_source_skb = globals()[PART1_FEATURE_TEST_DICT_NAME]\n",
    "    \n",
    "    for feature_name, X_train_orig in feature_data_train_source_skb.items():\n",
    "        X_val_orig = feature_data_val_source_skb.get(feature_name)\n",
    "        X_test_orig = feature_data_test_source_skb.get(feature_name)\n",
    "\n",
    "        current_y_train = y_train_final\n",
    "        current_y_val = y_val_final\n",
    "        current_y_test = y_test_final\n",
    "\n",
    "        if not isinstance(X_train_orig, np.ndarray) or X_train_orig.size == 0 or \\\n",
    "           (X_train_orig.ndim == 2 and X_train_orig.shape[0] != current_y_train.shape[0]) or \\\n",
    "           (X_train_orig.ndim == 1 and X_train_orig.shape[0] == 0 and current_y_train.shape[0] > 0):\n",
    "            print(f\"\\nSkipping SelectKBest for {feature_name}: Training features are empty or mismatched with labels.\")\n",
    "            continue\n",
    "\n",
    "        run_validation_skb = isinstance(X_val_orig, np.ndarray) and X_val_orig.size > 0 and \\\n",
    "                             isinstance(current_y_val, np.ndarray) and current_y_val.size > 0 and \\\n",
    "                             X_val_orig.shape[0] == current_y_val.shape[0]\n",
    "        if not run_validation_skb and isinstance(X_val_orig, np.ndarray) and X_val_orig.size > 0:\n",
    "             print(f\"Warning for SelectKBest {feature_name} validation: Label mismatch or empty labels. Validation eval will be skipped.\")\n",
    "\n",
    "        run_test_skb = isinstance(X_test_orig, np.ndarray) and X_test_orig.size > 0 and \\\n",
    "                       isinstance(current_y_test, np.ndarray) and current_y_test.size > 0 and \\\n",
    "                       X_test_orig.shape[0] == current_y_test.shape[0]\n",
    "        if not run_test_skb and isinstance(X_test_orig, np.ndarray) and X_test_orig.size > 0:\n",
    "            print(f\"Warning for SelectKBest {feature_name} test: Label mismatch or empty labels. Test eval will be skipped.\")\n",
    "            \n",
    "        print(f\"\\n===== Applying Feature Selection (SelectKBest) to {feature_name} Features =====\")\n",
    "        print(f\"Original Train X shape: {X_train_orig.shape}, y shape: {current_y_train.shape}\")\n",
    "\n",
    "        # 1. Scale original features before feature selection\n",
    "        scaler_for_skb = StandardScaler()\n",
    "        X_train_scaled_for_skb = scaler_for_skb.fit_transform(X_train_orig)\n",
    "        X_val_scaled_for_skb = scaler_for_skb.transform(X_val_orig) if run_validation_skb and X_val_orig.ndim == 2 else np.array([])\n",
    "        X_test_scaled_for_skb = scaler_for_skb.transform(X_test_orig) if run_test_skb and X_test_orig.ndim == 2 else np.array([])\n",
    "        \n",
    "        # 2. Apply SelectKBest\n",
    "        original_num_features_skb = X_train_scaled_for_skb.shape[1]\n",
    "        \n",
    "        if original_num_features_skb == 0:\n",
    "            print(f\"No features to select from for {feature_name} after scaling. Skipping SelectKBest.\")\n",
    "            continue\n",
    "\n",
    "        # Determine k: min(100, 50% of features, or original_num_features if very few)\n",
    "        if original_num_features_skb <= 10: \n",
    "            k_final_select_skb = original_num_features_skb\n",
    "        else:\n",
    "            k_half_select_skb = int(original_num_features_skb * 0.5)\n",
    "            k_final_select_skb = min(100, k_half_select_skb) \n",
    "            if k_final_select_skb == 0 and original_num_features_skb > 0 : k_final_select_skb = 1 # Ensure at least 1 feature\n",
    "\n",
    "        print(f\"Selecting top {k_final_select_skb} features from {feature_name} (original scaled: {original_num_features_skb})...\")\n",
    "        \n",
    "        try:\n",
    "            selector_skb = SelectKBest(score_func=f_classif, k=k_final_select_skb)\n",
    "            X_train_selectkbest = selector_skb.fit_transform(X_train_scaled_for_skb, current_y_train)\n",
    "            X_val_selectkbest = selector_skb.transform(X_val_scaled_for_skb) if run_validation_skb and X_val_scaled_for_skb.size > 0 else np.array([])\n",
    "            X_test_selectkbest = selector_skb.transform(X_test_scaled_for_skb) if run_test_skb and X_test_scaled_for_skb.size > 0 else np.array([])\n",
    "        except ValueError as e_skb: \n",
    "             print(f\"  SelectKBest Error for {feature_name}: {e_skb}. Possibly k > n_features. Trying k=min(k_final_select_skb, n_features).\")\n",
    "             try:\n",
    "                 k_fallback_skb_val = min(k_final_select_skb, X_train_scaled_for_skb.shape[1])\n",
    "                 if k_fallback_skb_val == 0 and X_train_scaled_for_skb.shape[1] > 0: k_fallback_skb_val = 1\n",
    "                 elif k_fallback_skb_val == 0:\n",
    "                     print(f\"  Cannot select 0 features for {feature_name} (original has {X_train_scaled_for_skb.shape[1]}). Skipping SelectKBest.\")\n",
    "                     continue\n",
    "                 selector_fallback_skb_inst = SelectKBest(score_func=f_classif, k=k_fallback_skb_val)\n",
    "                 X_train_selectkbest = selector_fallback_skb_inst.fit_transform(X_train_scaled_for_skb, current_y_train)\n",
    "                 X_val_selectkbest = selector_fallback_skb_inst.transform(X_val_scaled_for_skb) if run_validation_skb and X_val_scaled_for_skb.size > 0 else np.array([])\n",
    "                 X_test_selectkbest = selector_fallback_skb_inst.transform(X_test_scaled_for_skb) if run_test_skb and X_test_scaled_for_skb.size > 0 else np.array([])\n",
    "                 k_final_select_skb = k_fallback_skb_val \n",
    "             except Exception as e_fallback_skb_inner:\n",
    "                 print(f\"  SelectKBest Fallback Error for {feature_name}: {e_fallback_skb_inner}. Skipping this feature set for SelectKBest.\")\n",
    "                 continue\n",
    "        \n",
    "        print(f\"Shape after SelectKBest - Train: {X_train_selectkbest.shape}, Val: {X_val_selectkbest.shape if X_val_selectkbest.size > 0 else 'N/A'}, Test: {X_test_selectkbest.shape if X_test_selectkbest.size > 0 else 'N/A'}\")\n",
    "\n",
    "        # 3. Data was already scaled before selection.\n",
    "\n",
    "        # 4. Evaluate ML models on selected features\n",
    "        for model_name, model_instance_orig in ml_models.items(): # Using ml_models from Part 1\n",
    "            print(f\"\\n--- Training {model_name} with SelectKBest {feature_name} ---\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            current_model_skb = type(model_instance_orig)(**model_instance_orig.get_params())\n",
    "            if 'random_state' in current_model_skb.get_params(): current_model_skb.set_params(random_state=42)\n",
    "            if hasattr(current_model_skb, 'n_jobs') and model_name != \"Gaussian Naive Bayes\": current_model_skb.set_params(n_jobs=-1)\n",
    "            \n",
    "            try:\n",
    "                current_model_skb.fit(X_train_selectkbest, current_y_train)\n",
    "                train_time = time.time() - start_time\n",
    "\n",
    "                val_accuracy_skb = 0.0\n",
    "                val_report_dict_skb_default = {'macro avg': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0}, 'accuracy': 0.0}\n",
    "                if run_validation_skb and X_val_selectkbest.size > 0 :\n",
    "                    y_val_pred_skb = current_model_skb.predict(X_val_selectkbest)\n",
    "                    val_accuracy_skb = accuracy_score(current_y_val, y_val_pred_skb)\n",
    "                    labels_val_eval_skb = np.unique(np.concatenate((current_y_val, y_val_pred_skb)))\n",
    "                    target_names_val_eval_skb = [label_mapping.get(l, str(l)) for l in labels_val_eval_skb] if 'label_mapping' in globals() and label_mapping else [str(l) for l in labels_val_eval_skb]\n",
    "                    val_report_dict_skb = classification_report(current_y_val, y_val_pred_skb, target_names=target_names_val_eval_skb, labels=labels_val_eval_skb, output_dict=True, zero_division=0)\n",
    "                    print(f\"Validation Accuracy (SelectKBest): {val_accuracy_skb:.4f}\")\n",
    "                else:\n",
    "                    val_report_dict_skb = val_report_dict_skb_default\n",
    "                    if run_validation_skb: print(f\"Validation evaluation skipped for SelectKBest {feature_name} with {model_name} (empty X_val_selectkbest).\")\n",
    "\n",
    "                test_accuracy_skb = 0.0\n",
    "                test_report_str_skb = \"N/A (Test evaluation skipped)\"\n",
    "                test_report_dict_skb_default = {'macro avg': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0}, 'accuracy': 0.0}\n",
    "                test_time_skb = 0.0\n",
    "                if run_test_skb and X_test_selectkbest.size > 0:\n",
    "                    start_test_time = time.time()\n",
    "                    y_test_pred_skb = current_model_skb.predict(X_test_selectkbest)\n",
    "                    test_time_skb = time.time() - start_test_time\n",
    "                    \n",
    "                    test_accuracy_skb = accuracy_score(current_y_test, y_test_pred_skb)\n",
    "                    labels_test_eval_skb = np.unique(np.concatenate((current_y_test, y_test_pred_skb)))\n",
    "                    target_names_test_eval_skb = [label_mapping.get(l, str(l)) for l in labels_test_eval_skb] if 'label_mapping' in globals() and label_mapping else [str(l) for l in labels_test_eval_skb]\n",
    "\n",
    "                    test_report_str_skb = classification_report(current_y_test, y_test_pred_skb, target_names=target_names_test_eval_skb, labels=labels_test_eval_skb, zero_division=0)\n",
    "                    test_report_dict_skb = classification_report(current_y_test, y_test_pred_skb, target_names=target_names_test_eval_skb, labels=labels_test_eval_skb, output_dict=True, zero_division=0)\n",
    "                    print(f\"Test Accuracy (SelectKBest): {test_accuracy_skb:.4f}\")\n",
    "                    print(\"Test Set Classification Report (SelectKBest):\")\n",
    "                    print(test_report_str_skb)\n",
    "                else:\n",
    "                    test_report_dict_skb = test_report_dict_skb_default\n",
    "                    if run_test_skb: print(f\"Test evaluation skipped for SelectKBest {feature_name} with {model_name} (empty X_test_selectkbest).\")\n",
    "\n",
    "                results_part2_selectkbest.append({\n",
    "                    \"Feature Set\": f\"{feature_name}_SelectKBest_{k_final_select_skb}feat\",\n",
    "                    \"ML Model\": model_name,\n",
    "                    \"Validation Accuracy\": round(val_accuracy_skb, 4),\n",
    "                    \"Test Accuracy\": round(test_accuracy_skb, 4),\n",
    "                    \"Test Precision (macro)\": round(test_report_dict_skb['macro avg']['precision'], 4),\n",
    "                    \"Test Recall (macro)\": round(test_report_dict_skb['macro avg']['recall'], 4),\n",
    "                    \"Test F1-Score (macro)\": round(test_report_dict_skb['macro avg']['f1-score'], 4),\n",
    "                    \"Training Time (s)\": round(train_time, 2),\n",
    "                    \"Test Time (s)\": round(test_time_skb, 2)\n",
    "                })\n",
    "            except ValueError as ve_skb_model:\n",
    "                print(f\"ValueError training/evaluating {model_name} with SelectKBest {feature_name}: {ve_skb_model}\")\n",
    "            except Exception as e_skb_model:\n",
    "                print(f\"General error training/evaluating {model_name} with SelectKBest {feature_name}: {e_skb_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.4: Displaying Combined Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% --- Part 2: Displaying Combined Results ---\n",
    "print(\"\\n\\n--- Summary of Part 2 PCA Results ---\")\n",
    "df_results_part2_pca = pd.DataFrame(results_part2_pca)\n",
    "if not df_results_part2_pca.empty:\n",
    "    print(df_results_part2_pca.sort_values(by=\"Test Accuracy\", ascending=False).to_string())\n",
    "else:\n",
    "    print(\"No results to display for Part 2 PCA (results_part2_pca list is empty).\")\n",
    "\n",
    "print(\"\\n\\n--- Summary of Part 2 SelectKBest Results ---\")\n",
    "df_results_part2_selectkbest = pd.DataFrame(results_part2_selectkbest)\n",
    "if not df_results_part2_selectkbest.empty:\n",
    "    print(df_results_part2_selectkbest.sort_values(by=\"Test Accuracy\", ascending=False).to_string())\n",
    "else:\n",
    "    print(\"No results to display for Part 2 SelectKBest (results_part2_selectkbest list is empty).\")\n",
    "\n",
    "# For a combined comparison, you can concatenate with Part 1 results\n",
    "# Assuming results_part1_optimized is the LIST of DICTS from your updated Part 1\n",
    "# Or df_results_part1_optimized is the DATAFRAME from Part 1\n",
    "\n",
    "df_part1_for_concat_final = pd.DataFrame() # Initialize an empty DataFrame\n",
    "\n",
    "if 'df_results_part1_optimized' in globals() and isinstance(globals()['df_results_part1_optimized'], pd.DataFrame):\n",
    "    df_part1_for_concat_final = globals()['df_results_part1_optimized'].copy()\n",
    "    df_part1_for_concat_final[\"Processing_Type\"] = \"Original\" # Add a type column\n",
    "elif 'results_part1_optimized' in globals() and isinstance(globals()['results_part1_optimized'], list):\n",
    "    df_part1_for_concat_final = pd.DataFrame(globals()['results_part1_optimized'])\n",
    "    if not df_part1_for_concat_final.empty:\n",
    "         df_part1_for_concat_final[\"Processing_Type\"] = \"Original\"\n",
    "else:\n",
    "    print(\"Warning: Part 1 results (results_part1_optimized list or df_results_part1_optimized DataFrame) not found for combined display.\")\n",
    "\n",
    "# Prepare Part 2 DataFrames for concatenation by adding a distinguishing column or modifying \"Feature Set\"\n",
    "df_results_part2_pca_display = df_results_part2_pca.copy()\n",
    "if not df_results_part2_pca_display.empty:\n",
    "    df_results_part2_pca_display[\"Processing_Type\"] = \"PCA\"\n",
    "\n",
    "df_results_part2_skb_display = df_results_part2_selectkbest.copy()\n",
    "if not df_results_part2_skb_display.empty:\n",
    "    df_results_part2_skb_display[\"Processing_Type\"] = \"SelectKBest\"\n",
    "\n",
    "\n",
    "all_results_dfs_final_combined = []\n",
    "if not df_part1_for_concat_final.empty:\n",
    "    all_results_dfs_final_combined.append(df_part1_for_concat_final)\n",
    "if not df_results_part2_pca_display.empty:\n",
    "    all_results_dfs_final_combined.append(df_results_part2_pca_display)\n",
    "if not df_results_part2_skb_display.empty:\n",
    "    all_results_dfs_final_combined.append(df_results_part2_skb_display)\n",
    "\n",
    "if all_results_dfs_final_combined:\n",
    "    df_combined_all_parts = pd.concat(all_results_dfs_final_combined, ignore_index=True)\n",
    "    print(\"\\n\\n--- Combined Summary of Results (Part 1 Original, Part 2 PCA, Part 2 SelectKBest) ---\")\n",
    "    # Sort by Feature Set (original name) and then by Test Accuracy for easier comparison\n",
    "    # We might need to extract original feature name from the modified \"Feature Set\" column\n",
    "    if \"Feature Set\" in df_combined_all_parts.columns:\n",
    "        df_combined_all_parts[\"Original_Feature_Set\"] = df_combined_all_parts[\"Feature Set\"].apply(lambda x: x.split('_')[0] if isinstance(x,str) else \"Unknown\")\n",
    "        print(df_combined_all_parts.sort_values(by=[\"Original_Feature_Set\", \"Test Accuracy\"], ascending=[True, False]).to_string())\n",
    "    else:\n",
    "        print(df_combined_all_parts.sort_values(by=[\"Test Accuracy\"], ascending=[False]).to_string()) # Fallback sort\n",
    "\n",
    "else:\n",
    "    print(\"No results from any part to combine for the final display.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Part 2: Comments and Interpretations\n",
    "#\n",
    "# * **PCA Application:**\n",
    "#     * For each feature set (HSV, HoG, Gabor), how many principal components were selected to retain 95% variance? Does this number make sense given the original dimensionality and nature of the features? (e.g., \"HoG had an original dimensionality of X, and PCA reduced it to Y components, indicating significant redundancy or correlation in the original HoG features.\")\n",
    "#     * How did classification performance with PCA-transformed features compare to using the original scaled features (from Part 1)? Did PCA improve, degrade, or have a mixed effect on accuracy/F1-score for different ML models? (e.g., \"For SVM, PCA on HSV features improved accuracy by Z%, but for Random Forest, it decreased slightly.\")\n",
    "#     * Discuss any significant changes in training/testing times when using PCA. (e.g., \"Training time for all models was notably faster with PCA-transformed features due to the lower dimensionality.\")\n",
    "# * **Feature Selection (SelectKBest with f_classif):**\n",
    "#     * For each feature set, how many features were selected (k)? How does this compare to the original and PCA dimensions?\n",
    "#     * How did performance with the selected features compare to the original scaled features and the PCA-transformed features? (e.g., \"SelectKBest on Gabor features, while reducing features by half, maintained comparable accuracy to the original Gabor set and outperformed PCA for the Logistic Regression model.\")\n",
    "#     * Discuss any significant changes in training/testing times when using feature selection.\n",
    "# * **Overall Comparison:**\n",
    "#     * Which approach (original, PCA, or feature selection) yielded the best balance of performance and efficiency (training/test time) for each feature type and each ML algorithm?\n",
    "#     * Were there any feature types that benefited more from PCA or feature selection than others? Why might this be? (e.g., \"Color-based features (HSV) might have seen less improvement from PCA if color channels were already relatively independent, whereas texture features like Gabor might have more correlated components that PCA could effectively reduce.\")\n",
    "#     * Based on your results, what can you conclude about the utility of PCA and feature selection for this specific image classification task using these traditional features? Are they always beneficial? When might one be preferred over the other?\n",
    "#\n",
    "# *(Please fill in your detailed comments and interpretations here based on the results you obtain.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_combined_all_parts_final' in globals() and not df_combined_all_parts.empty:\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    # Use 'Original_Feature_Set_Name' for grouping by HSV, HoG, Gabor\n",
    "    # Use 'Processing_Type' for hue\n",
    "    sns.barplot(x=\"ML Model\", y=\"Test Accuracy\", hue=\"Processing_Type\", \n",
    "                data=df_combined_all_parts, palette=\"muted\",\n",
    "                # Optional: use facet grid if you want separate plots per Original_Feature_Set_Name\n",
    "                # col=\"Original_Feature_Set_Name\" \n",
    "               )\n",
    "    plt.title(\"Overall Model Test Accuracy: Original vs PCA vs SelectKBest\", fontsize=18)\n",
    "    plt.ylabel(\"Test Accuracy\", fontsize=14)\n",
    "    plt.xlabel(\"Machine Learning Model\", fontsize=14)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend(title=\"Processing Type\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"df_combined_all_parts_final DataFrame not found or is empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
